{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a8bbcb8",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd29870",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to train custom openWakeWord models using pre-defined datasets and an automated process for dataset generation and training. While not guaranteed to always produce the best performing model, the methods shown in this notebook often produce baseline models with releatively strong performance.\n",
    "\n",
    "Manual data preparation and model training (e.g., see the [training models](training_models.ipynb) notebook) remains an option for when full control over the model development process is needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cebb7c",
   "metadata": {},
   "source": [
    "# Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bd1f49",
   "metadata": {},
   "source": [
    "To begin, we'll need to install the requirements for training custom models. In particular, a relatively recent version of Pytorch and custom fork of the [piper-sample-generator](https://github.com/dscripka/piper-sample-generator) library for generating synthetic examples for the custom model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645f5330",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Environment setup\n",
    "\n",
    "# install piper-sample-generator\n",
    "!git clone https://github.com/dscripka/piper-sample-generator\n",
    "!wget -O models/en-us-libritts-high.pt 'https://github.com/rhasspy/piper-sample-generator/releases/download/v1.0.0/en-us-libritts-high.pt'\n",
    "\n",
    "# install openwakeword (full installation to support training)\n",
    "!pip install openwakeword[full]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "259e6491",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-04T02:00:48.344884Z",
     "start_time": "2023-09-04T02:00:48.340514Z"
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from openwakeword.data import mmap_batch_generator, generate_adversarial_texts\n",
    "from openwakeword.utils import compute_features_from_generator\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import uuid\n",
    "import yaml\n",
    "\n",
    "# Set paths for locally installed piper-sample-generator\n",
    "sys.path.insert(0, \"../../piper-sample-generator/\")\n",
    "from generate_samples import generate_samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb69d8e4",
   "metadata": {},
   "source": [
    "# Download Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4434c8",
   "metadata": {},
   "source": [
    "When training new openWakeWord models using the automated procedure, four specific types of data are required:\n",
    "\n",
    "1) Synthetic examples of the target word/phrase generated with text-to-speech models\n",
    "\n",
    "2) Synthetic examples of adversarial words/phrases generated with text-to-speech models\n",
    "\n",
    "3) Room impulse reponses and noise/background audio data to augment the synthetic examples and make them more realistic\n",
    "\n",
    "4) Generic \"negative\" audio data that is very unlikely to contain examples of the target word/phrase in the context where the model should detect it. This data can be the original audio data, or precomputed openWakeWord features ready for model training.\n",
    "\n",
    "5) Validation data to use for early-stopping when training the model.\n",
    "\n",
    "For the purposes of this notebook, all five of these sources can be obtained from HuggingFace thanks to their excellent `datasets` library and extremely generous hosting policy. Also note that only a portion of some datasets are downloaded. But for the best possible performance, you are encouraged to download the entire dataset and keep a local copy for future training runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ed7bacd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-04T01:07:17.746749Z",
     "start_time": "2023-09-04T01:07:17.740846Z"
    }
   },
   "outputs": [],
   "source": [
    "# Download room impulse responses\n",
    "\n",
    "output_dir = \"./mit_rirs\"\n",
    "os.mkdir(output_dir) if not os.path.exists(output_dir)\n",
    "rir_dataset = datasets.load_dataset(\"davidscripka/MIT_environmental_impulse_responses\", split=\"train\", streaming=True)\n",
    "\n",
    "for row in tqdm(rir_dataset):\n",
    "    name = row['audio']['path'].split('/')[-1]\n",
    "    scipy.io.wavfile.write(os.path.join(output_dir, name), 16000, row['audio']['array'])\n",
    "    i += 1\n",
    "    if i == n_total:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4532caf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Download noise and background audio\n",
    "\n",
    "# FSD50k Noise Dataset (warning, this can take 5 minutes to prepare when streaming)\n",
    "# https://zenodo.org/record/4060432\n",
    "output_dir = \"./fsd50k\"\n",
    "os.mkdir(output_dir) if not os.path.exists(output_dir)\n",
    "fsd50k_dataset = datasets.load_dataset(\"Fhrozen/FSD50k\", split=\"validation\", streaming=True)  # ~40,000 files in this split\n",
    "fsd50k_dataset = iter(fsd50k_dataset.cast_column(\"audio\", datasets.Audio(sampling_rate=16000)))\n",
    "\n",
    "n_total = 500  # use only 500 clips for this example notebook, reccomend increasing for full-scale training\n",
    "for i in tqdm(range(n_total)):\n",
    "    row = next(fsd50k_dataset)\n",
    "    name = row['audio']['path'].split('/')[-1]\n",
    "    scipy.io.wavfile.write(os.path.join(output_dir, name), 16000, row['audio']['array'])\n",
    "    i += 1\n",
    "    if i == n_total:\n",
    "        break\n",
    "\n",
    "# Free Music Archive dataset\n",
    "# https://github.com/mdeff/fma\n",
    "\n",
    "output_dir = \"./fma\"\n",
    "os.mkdir(output_dir) if not os.path.exists(output_dir)\n",
    "fma_dataset = datasets.load_dataset(\"rudraml/fma\", name=\"small\", split=\"train\", streaming=True)\n",
    "fma_dataset = iter(fma_dataset.cast_column(\"audio\", datasets.Audio(sampling_rate=16000)))\n",
    "\n",
    "n_hours = 1\n",
    "for i in tqdm(range(n_hours*3600//30)):  # this works because the FMA dataset is all 30 second clips\n",
    "    row = next(fma_dataset)\n",
    "    name = row['audio']['path'].split('/')[-1]\n",
    "    scipy.io.wavfile.write(os.path.join(output_dir, name), 16000, row['audio']['array'])\n",
    "    i += 1\n",
    "    if i == n_hours*3600//30:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a475459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download pre-computed openWakeWord features for training and validation\n",
    "\n",
    "# training set (~2,000 hours)\n",
    "!wget https://huggingface.co/datasets/davidscripka/openwakeword_features/blob/main/openwakeword_features_ACAV100M_2000_hrs_16bit.npy\n",
    "\n",
    "# validation set (~11 hours)\n",
    "!wget https://huggingface.co/datasets/davidscripka/openwakeword_features/blob/main/validation_set_features.npy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda8e47e",
   "metadata": {},
   "source": [
    "# Define Training Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2013a4",
   "metadata": {},
   "source": [
    "For automated model training openWakeWord uses a specially designed training script and a [YAML](https://yaml.org/) configuration file that defines all of the information required for training a new wake word/phrase detection model.\n",
    "\n",
    "It is strongly recommended that you review [the example config file](../examples/custom_model.yml), as each value is fully documented there. For the purposes of this notebook, we'll read in the YAML file to modify certain configuration parameters before saving a new YAML file for training our example model. Specifically:\n",
    "\n",
    "- We'll train a detection model for the phrase \"hey sebastian\"\n",
    "- We'll only generate 5,000 positive and negative examples (to save on time for this example)\n",
    "- We'll only generate 1,000 validation positive and negative examples for early stopping (again to save time)\n",
    "- The model will be trained for 30,000 steps (larger datasets will benefit from longer training)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "81bc1bea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-04T02:03:46.688266Z",
     "start_time": "2023-09-04T02:03:46.672580Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name': 'my_model',\n",
       " 'target_phrase': ['hey jarvis'],\n",
       " 'total_length': 32000,\n",
       " 'custom_negative_phrases': [],\n",
       " 'n_samples': 10000,\n",
       " 'n_samples_val': 2000,\n",
       " 'tts_batch_size': 50,\n",
       " 'augmentation_batch_size': 16,\n",
       " 'piper_sample_generator_path': './piper-sample-generator',\n",
       " 'output_dir': './generated_data',\n",
       " 'rir_paths': ['./mit_rirs'],\n",
       " 'background_paths': ['./background_clips'],\n",
       " 'false_positive_validation_data_path': './validation_set_features.npy',\n",
       " 'augmentation_rounds': 1,\n",
       " 'feature_data_files': {'ACAV100M_sample': './openwakeword_features_ACAV100M_2000_hrs_16bit.npy'},\n",
       " 'batch_n_per_class': {'ACAV100M_sample': 1024,\n",
       "  'adversarial_negative': 50,\n",
       "  'positive': 50},\n",
       " 'model_type': 'dnn',\n",
       " 'layer_size': 32,\n",
       " 'steps': 100000,\n",
       " 'max_negative_weight': 1500,\n",
       " 'target_accuracy': 0.7,\n",
       " 'target_recall': 0.5,\n",
       " 'target_false_positives_per_hour': 0.2}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load YAML file\n",
    "config = yaml.load(open(\"../examples/custom_model.yml\", 'r').read(), yaml.Loader)\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d0af4242",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-04T02:30:24.194893Z",
     "start_time": "2023-09-04T02:30:24.176938Z"
    }
   },
   "outputs": [],
   "source": [
    "# Modify values in the config and save a new version\n",
    "\n",
    "config[\"target_phrase\"] = [\"hey sebastian\"]\n",
    "config[\"n_samples\"] = 5000\n",
    "config[\"n_samples_val\"] = 1000\n",
    "config[\"steps\"] = 30000\n",
    "\n",
    "with open('my_model.yaml', 'w') as file:\n",
    "    documents = yaml.dump(config, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db52159f",
   "metadata": {},
   "source": [
    "# Start Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8d66c5",
   "metadata": {},
   "source": [
    "With the data downloaded and training configuration set, we can now start training the model. We'll do this in parts to better illustrate the sequence, but you can also execute every step sequentially for a fully automated process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dff83ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Generate synthetic clips\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openwakeword_dev",
   "language": "python",
   "name": "openwakeword_dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
