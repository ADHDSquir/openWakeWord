{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1970bd02",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62fa8fb7",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to train custom openWakeWord models using pre-defined datasets, an automated process for synthetic data generation/augmentation, and a custom training process. While not guaranteed to always produce the best performing model, the methods shown in this notebook often produce baseline models with relatively strong performance.\n",
    "\n",
    "Manual data preparation and model training (e.g., see the [training models](training_models.ipynb) notebook) remains an option for when full control over the model development process is needed.\n",
    "\n",
    "At a high level, the automatic training process takes advantages of several techniques to try and produce a good model, including:\n",
    "\n",
    "- Early-stopping and checkpoint averaging (similar to [stochastic weight averaging](https://arxiv.org/abs/1803.05407)) to search for the best models found during training, according to the validation data\n",
    "- Variable learning rates with cosine decay and multiple cycles\n",
    "- Adaptive batch construction to focus on only high-loss examples when the model begins to converge, combined with gradient accumulation to ensure that batch sizes are still large enough for stable training\n",
    "- Cycical weight schedules for negative examples to help the model reduce false-positive rates\n",
    "\n",
    "See the contents of the `train.py` file for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af4f86d",
   "metadata": {},
   "source": [
    "# Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbbd9f0",
   "metadata": {},
   "source": [
    "To begin, we'll need to install the requirements for training custom models. In particular, a relatively recent version of Pytorch and custom fork of the [piper-sample-generator](https://github.com/dscripka/piper-sample-generator) library for generating synthetic examples for the custom model.\n",
    "\n",
    "**Important Note!** Currently, automated model training is only supported on linux systems due to the requirements of the text to speech library used for synthetic sample generation (Piper). It may be possible to use Piper on Windows/Mac systems, but that has not (yet) been tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbc61cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Environment setup\n",
    "\n",
    "# install piper-sample-generator (currently only supports linux systems)\n",
    "!git clone https://github.com/dscripka/piper-sample-generator\n",
    "!wget -O piper-sample-generator/models/en-us-libritts-high.pt 'https://github.com/rhasspy/piper-sample-generator/releases/download/v1.0.0/en-us-libritts-high.pt'\n",
    "!apt-get install libespeak-ng1  # may not be required on all systems\n",
    "!pip install espeak_phonemizer\n",
    "\n",
    "# install openwakeword (full installation to support training)\n",
    "!git clone https://github.com/dscripka/openwakeword\n",
    "!pip install -e ./openwakeword[full]\n",
    "!cd openwakeword\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8db5c9e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-04T13:42:01.183840Z",
     "start_time": "2023-09-04T13:41:59.752153Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dscripka/anaconda3/envs/openwakeword_dev/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from openwakeword.data import mmap_batch_generator, generate_adversarial_texts\n",
    "from openwakeword.utils import compute_features_from_generator\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import uuid\n",
    "import yaml\n",
    "import datasets\n",
    "import scipy\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4bbe15",
   "metadata": {},
   "source": [
    "# Download Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24580c3e",
   "metadata": {},
   "source": [
    "When training new openWakeWord models using the automated procedure, four specific types of data are required:\n",
    "\n",
    "1) Synthetic examples of the target word/phrase generated with text-to-speech models\n",
    "\n",
    "2) Synthetic examples of adversarial words/phrases generated with text-to-speech models\n",
    "\n",
    "3) Room impulse reponses and noise/background audio data to augment the synthetic examples and make them more realistic\n",
    "\n",
    "4) Generic \"negative\" audio data that is very unlikely to contain examples of the target word/phrase in the context where the model should detect it. This data can be the original audio data, or precomputed openWakeWord features ready for model training.\n",
    "\n",
    "5) Validation data to use for early-stopping when training the model.\n",
    "\n",
    "For the purposes of this notebook, all five of these sources will either be generated manually or can be obtained from HuggingFace thanks to their excellent `datasets` library and extremely generous hosting policy. Also note that while only a portion of some datasets are downloaded, for the best possible performance it is recommended to download the entire dataset and keep a local copy for future training runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "37feb7f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-04T01:07:17.746749Z",
     "start_time": "2023-09-04T01:07:17.740846Z"
    }
   },
   "outputs": [],
   "source": [
    "# Download room impulse responses collected by MIT\n",
    "# https://mcdermottlab.mit.edu/Reverb/IR_Survey.html\n",
    "\n",
    "output_dir = \"./mit_rirs\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "rir_dataset = datasets.load_dataset(\"davidscripka/MIT_environmental_impulse_responses\", split=\"train\", streaming=True)\n",
    "\n",
    "for row in tqdm(rir_dataset):\n",
    "    name = row['audio']['path'].split('/')[-1]\n",
    "    scipy.io.wavfile.write(os.path.join(output_dir, name), 16000, row['audio']['array'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e16363e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Download noise and background audio\n",
    "\n",
    "# FSD50k Noise Dataset (warning, this can take 5 minutes to prepare when streaming)\n",
    "# https://zenodo.org/record/4060432\n",
    "output_dir = \"./fsd50k\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "fsd50k_dataset = datasets.load_dataset(\"Fhrozen/FSD50k\", split=\"validation\", streaming=True)  # ~40,000 files in this split\n",
    "fsd50k_dataset = iter(fsd50k_dataset.cast_column(\"audio\", datasets.Audio(sampling_rate=16000)))\n",
    "\n",
    "n_total = 500  # use only 500 clips for this example notebook, recommend increasing for full-scale training\n",
    "for i in tqdm(range(n_total)):\n",
    "    row = next(fsd50k_dataset)\n",
    "    name = row['audio']['path'].split('/')[-1]\n",
    "    scipy.io.wavfile.write(os.path.join(output_dir, name), 16000, row['audio']['array'])\n",
    "    i += 1\n",
    "    if i == n_total:\n",
    "        break\n",
    "\n",
    "# Free Music Archive dataset\n",
    "# https://github.com/mdeff/fma\n",
    "\n",
    "output_dir = \"./fma\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "fma_dataset = datasets.load_dataset(\"rudraml/fma\", name=\"small\", split=\"train\", streaming=True)\n",
    "fma_dataset = iter(fma_dataset.cast_column(\"audio\", datasets.Audio(sampling_rate=16000)))\n",
    "\n",
    "n_hours = 1  # use only 1 hour of clips for this example notebook, recommend increasing for full-scale training\n",
    "for i in tqdm(range(n_hours*3600//30)):  # this works because the FMA dataset is all 30 second clips\n",
    "    row = next(fma_dataset)\n",
    "    name = row['audio']['path'].split('/')[-1]\n",
    "    scipy.io.wavfile.write(os.path.join(output_dir, name), 16000, row['audio']['array'])\n",
    "    i += 1\n",
    "    if i == n_hours*3600//30:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a500d6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download pre-computed openWakeWord features for training and validation\n",
    "\n",
    "# training set (~2,000 hours)\n",
    "!wget https://huggingface.co/datasets/davidscripka/openwakeword_features/blob/main/openwakeword_features_ACAV100M_2000_hrs_16bit.npy\n",
    "\n",
    "# validation set for false positive rate estimation (~11 hours)\n",
    "!wget https://huggingface.co/datasets/davidscripka/openwakeword_features/blob/main/validation_set_features.npy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9ba30f",
   "metadata": {},
   "source": [
    "# Define Training Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1258ea70",
   "metadata": {},
   "source": [
    "For automated model training openWakeWord uses a specially designed training script and a [YAML](https://yaml.org/) configuration file that defines all of the information required for training a new wake word/phrase detection model.\n",
    "\n",
    "It is strongly recommended that you review [the example config file](../examples/custom_model.yml), as each value is fully documented there. For the purposes of this notebook, we'll read in the YAML file to modify certain configuration parameters before saving a new YAML file for training our example model. Specifically:\n",
    "\n",
    "- We'll train a detection model for the phrase \"hey sebastian\"\n",
    "- We'll only generate 5,000 positive and negative examples (to save on time for this example)\n",
    "- We'll only generate 1,000 validation positive and negative examples for early stopping (again to save time)\n",
    "- The model will only be trained for 10,000 steps (larger datasets will benefit from longer training)\n",
    "- We'll reduce the target metrics to account for the small dataset size and limited training.\n",
    "\n",
    "On the topic of target metrics, there are *not* specific guidelines about what these metrics should be in practice, and you will need to conduct testing in your target deployment environment to establish good thresholds. However, from very limited testing the default values in the config file (accuracy >= 0.7, recall >= 0.5, false-positive rate <= 0.2 per hour) seem to produce model with reasonable performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e4ea6bcd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-04T18:11:33.893397Z",
     "start_time": "2023-09-04T18:11:33.878938Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name': 'my_model',\n",
       " 'target_phrase': ['hey jarvis'],\n",
       " 'total_length': 32000,\n",
       " 'custom_negative_phrases': [],\n",
       " 'n_samples': 10000,\n",
       " 'n_samples_val': 2000,\n",
       " 'tts_batch_size': 50,\n",
       " 'augmentation_batch_size': 16,\n",
       " 'piper_sample_generator_path': './piper-sample-generator',\n",
       " 'output_dir': './generated_data',\n",
       " 'rir_paths': ['./mit_rirs'],\n",
       " 'background_paths': ['./background_clips'],\n",
       " 'false_positive_validation_data_path': './validation_set_features.npy',\n",
       " 'augmentation_rounds': 1,\n",
       " 'feature_data_files': {'ACAV100M_sample': './openwakeword_features_ACAV100M_2000_hrs_16bit.npy'},\n",
       " 'batch_n_per_class': {'ACAV100M_sample': 1024,\n",
       "  'adversarial_negative': 50,\n",
       "  'positive': 50},\n",
       " 'model_type': 'dnn',\n",
       " 'layer_size': 32,\n",
       " 'steps': 100000,\n",
       " 'max_negative_weight': 1500,\n",
       " 'target_accuracy': 0.7,\n",
       " 'target_recall': 0.5,\n",
       " 'target_false_positives_per_hour': 0.2}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load default YAML config file for training\n",
    "config = yaml.load(open(\"openwakeword/examples/custom_model.yml\", 'r').read(), yaml.Loader)\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "026fa5cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-04T15:07:00.859210Z",
     "start_time": "2023-09-04T15:07:00.841472Z"
    }
   },
   "outputs": [],
   "source": [
    "# Modify values in the config and save a new version\n",
    "\n",
    "config[\"target_phrase\"] = [\"hey sebastian\"]\n",
    "config[\"n_samples\"] = 1000\n",
    "config[\"n_samples_val\"] = 1000\n",
    "config[\"steps\"] = 10000\n",
    "config[\"target_accuracy\"] = 0.6\n",
    "config[\"target_recall\"] = 0.25\n",
    "\n",
    "config[\"background_paths\"] = []#['./fsd50k', './fma']\n",
    "config[\"false_positive_validation_data_path\"] = \"val_set_features.npy\"\n",
    "config[\"feature_data_files\"] = {\"ACAV100M_sample\": \"openwakeword_features_ACAV100M_2000_hrs_16bit.npy\"}\n",
    "\n",
    "with open('my_model.yaml', 'w') as file:\n",
    "    documents = yaml.dump(config, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fa8b05",
   "metadata": {},
   "source": [
    "# Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f8bb92",
   "metadata": {},
   "source": [
    "With the data downloaded and training configuration set, we can now start training the model. We'll do this in parts to better illustrate the sequence, but you can also execute every step at once for a fully automated process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dab81483",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-04T13:50:08.803326Z",
     "start_time": "2023-09-04T13:50:06.790241Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:Generating positive clips for training\r\n",
      "WARNING:root:Skipping generation of positive clips for training, as ~1000 already exist\r\n",
      "INFO:root:Generating positive clips for testing\r\n",
      "WARNING:root:Skipping generation of positive clips testing, as ~1000 already exist\r\n",
      "INFO:root:Generating negative clips for training\r\n",
      "WARNING:root:Skipping generation of negative clips for training, as ~1000 already exist\r\n",
      "INFO:root:Generating negative clips for testing\r\n",
      "WARNING:root:Skipping generation of negative clips for testing, as ~1000 already exist\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Step 1: Generate synthetic clips\n",
    "# For the number of clips we are using, this should take ~10 minutes on a free Google Colab instance\n",
    "# If generation fails, you can simply run this command again as it will continue generating until the\n",
    "# number of files meets the targets specified in the config file\n",
    "\n",
    "!{sys.executable} openwakeword/openwakeword/train.py --training_config my_model.yaml --generate_clips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7ef78386",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-04T13:56:08.781018Z",
     "start_time": "2023-09-04T13:55:40.203515Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:Creating augmentation generators\n",
      "INFO:root:Computing openwakeword features for generated samples\n",
      "Computing features: 100%|███████████████████████| 62/62 [00:05<00:00, 10.58it/s]\n",
      "Trimming empty rows: 1it [00:00,  7.99it/s]\n",
      "Computing features: 100%|███████████████████████| 62/62 [00:05<00:00, 10.45it/s]\n",
      "Trimming empty rows: 1it [00:00,  8.07it/s]\n",
      "Computing features: 100%|███████████████████████| 62/62 [00:05<00:00, 10.77it/s]\n",
      "Trimming empty rows: 1it [00:00,  8.09it/s]\n",
      "Computing features: 100%|███████████████████████| 62/62 [00:05<00:00, 10.53it/s]\n",
      "Trimming empty rows: 1it [00:00,  8.07it/s]\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Step 2: Augment the generated clips\n",
    "\n",
    "!{sys.executable} openwakeword/openwakeword/train.py --training_config my_model.yaml --augment_clips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e2acb50a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-04T15:11:14.742260Z",
     "start_time": "2023-09-04T15:07:03.755159Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:Starting training sequence 1...\n",
      "Training:  75%|████████████████████▏      | 7496/10000 [00:45<00:14, 168.37it/s]1.3274336 0.66 0.331\n",
      "INFO:root:Saving checkpoint with metrics >= to targets!\n",
      "Training:  76%|█████████████████████▎      | 7621/10000 [00:49<00:28, 82.33it/s]1.3274336 0.66 0.331\n",
      "INFO:root:Saving checkpoint with metrics >= to targets!\n",
      "Training:  78%|█████████████████████▋      | 7762/10000 [00:53<00:22, 97.84it/s]2.300885 0.6735 0.362\n",
      "Training:  79%|██████████████████████      | 7887/10000 [00:57<00:25, 83.36it/s]0.088495575 0.607 0.216\n",
      "INFO:root:Saving checkpoint with metrics >= to targets!\n",
      "Training:  80%|██████████████████████▍     | 8011/10000 [01:00<00:23, 83.46it/s]2.2123895 0.676 0.367\n",
      "Training:  82%|██████████████████████▊     | 8153/10000 [01:04<00:19, 96.94it/s]0.26548672 0.623 0.252\n",
      "Training:  83%|███████████████████████▏    | 8277/10000 [01:08<00:20, 82.58it/s]0.088495575 0.605 0.212\n",
      "INFO:root:Saving checkpoint with metrics >= to targets!\n",
      "Training:  84%|███████████████████████▌    | 8416/10000 [01:12<00:16, 95.46it/s]0.44247788 0.6405 0.29\n",
      "Training:  85%|███████████████████████▉    | 8541/10000 [01:16<00:17, 82.74it/s]1.5044248 0.6605 0.335\n",
      "Training:  87%|████████████████████████▎   | 8683/10000 [01:19<00:13, 96.74it/s]1.2389381 0.6595 0.333\n",
      "Training:  88%|████████████████████████▋   | 8807/10000 [01:23<00:14, 82.40it/s]0.44247788 0.638 0.284\n",
      "Training:  89%|█████████████████████████   | 8931/10000 [01:27<00:12, 82.28it/s]0.44247788 0.643 0.295\n",
      "Training:  91%|█████████████████████████▍  | 9073/10000 [01:31<00:09, 95.54it/s]0.9734513 0.658 0.327\n",
      "Training:  92%|█████████████████████████▊  | 9197/10000 [01:34<00:09, 83.55it/s]1.5929203 0.6635 0.341\n",
      "Training:  93%|██████████████████████████▏ | 9338/10000 [01:38<00:06, 96.77it/s]2.0353982 0.6685 0.352\n",
      "Training:  95%|██████████████████████████▍ | 9463/10000 [01:42<00:06, 82.58it/s]1.1504425 0.658 0.33\n",
      "Training:  96%|██████████████████████████▉ | 9605/10000 [01:46<00:04, 97.06it/s]1.2389381 0.6585 0.331\n",
      "Training:  97%|███████████████████████████▏| 9727/10000 [01:49<00:03, 81.79it/s]1.2389381 0.6585 0.331\n",
      "Training:  99%|███████████████████████████▌| 9866/10000 [01:53<00:01, 95.62it/s]1.1504425 0.6585 0.33\n",
      "Training: 100%|███████████████████████████▉| 9999/10000 [01:57<00:00, 85.19it/s]\n",
      "INFO:root:Starting training sequence 2...\n",
      "Training:   4%|█▏                           | 41/1000.0 [00:00<00:11, 86.52it/s]1.1504425 0.658 0.33\n",
      "Training:   9%|██▌                          | 89/1000.0 [00:04<00:37, 24.32it/s]1.4159292 0.6595 0.333\n",
      "Training:  14%|████                        | 143/1000.0 [00:07<00:36, 23.68it/s]1.3274336 0.6585 0.331\n",
      "Training:  20%|█████▍                      | 195/1000.0 [00:10<00:34, 23.10it/s]1.5044248 0.6605 0.335\n",
      "Training:  25%|██████▉                     | 247/1000.0 [00:14<00:32, 22.84it/s]1.6814159 0.665 0.344\n",
      "Training:  30%|████████▍                   | 300/1000.0 [00:17<00:30, 22.77it/s]1.858407 0.668 0.352\n",
      "Training:  35%|█████████▉                  | 353/1000.0 [00:20<00:27, 23.39it/s]2.1238937 0.671 0.359\n",
      "Training:  40%|███████████▎                | 405/1000.0 [00:24<00:27, 22.00it/s]1.858407 0.6685 0.352\n",
      "Training:  46%|████████████▊               | 458/1000.0 [00:27<00:23, 23.08it/s]1.4159292 0.6605 0.335\n",
      "Training:  51%|██████████████▎             | 512/1000.0 [00:30<00:20, 23.52it/s]1.5929203 0.664 0.342\n",
      "Training:  56%|███████████████▊            | 563/1000.0 [00:33<00:19, 22.59it/s]1.1504425 0.659 0.33\n",
      "Training:  62%|█████████████████▏          | 616/1000.0 [00:37<00:17, 22.47it/s]0.44247788 0.641 0.291\n",
      "Training:  67%|██████████████████▋         | 667/1000.0 [00:40<00:14, 22.93it/s]0.44247788 0.6415 0.292\n",
      "Training:  74%|████████████████████▋       | 737/1000.0 [00:43<00:08, 30.03it/s]0.44247788 0.6435 0.296\n",
      "Training:  79%|██████████████████████      | 787/1000.0 [00:47<00:08, 23.88it/s]0.53097343 0.646 0.301\n",
      "Training:  84%|███████████████████████▍    | 837/1000.0 [00:50<00:07, 22.92it/s]0.53097343 0.646 0.301\n",
      "Training:  89%|████████████████████████▊   | 886/1000.0 [00:53<00:05, 22.13it/s]0.53097343 0.646 0.301\n",
      "Training:  94%|██████████████████████████▏ | 935/1000.0 [00:57<00:02, 22.19it/s]0.53097343 0.6465 0.303\n",
      "Training: 100%|███████████████████████████▉| 999/1000.0 [01:00<00:00, 16.52it/s]\n",
      "INFO:root:Starting training sequence 3...\n",
      "Training:   5%|█▎                           | 47/1000.0 [00:00<00:09, 97.00it/s]0.53097343 0.6465 0.303\n",
      "Training:  10%|██▊                          | 95/1000.0 [00:04<00:35, 25.72it/s]0.53097343 0.6465 0.303\n",
      "Training:  14%|████                        | 145/1000.0 [00:07<00:36, 23.53it/s]0.53097343 0.6465 0.303\n",
      "Training:  20%|█████▍                      | 195/1000.0 [00:10<00:35, 22.67it/s]0.53097343 0.6465 0.303\n",
      "Training:  25%|██████▉                     | 247/1000.0 [00:14<00:34, 22.09it/s]0.53097343 0.647 0.304\n",
      "Training:  30%|████████▎                   | 299/1000.0 [00:17<00:30, 22.78it/s]0.7079646 0.6485 0.308\n",
      "Training:  35%|█████████▉                  | 353/1000.0 [00:20<00:28, 22.88it/s]0.7079646 0.6485 0.308\n",
      "Training:  41%|███████████▎                | 406/1000.0 [00:23<00:25, 23.08it/s]0.7079646 0.649 0.309\n",
      "Training:  46%|████████████▊               | 457/1000.0 [00:27<00:23, 22.67it/s]0.7079646 0.6495 0.31\n",
      "Training:  51%|██████████████▎             | 511/1000.0 [00:30<00:21, 23.20it/s]0.7079646 0.65 0.311\n",
      "Training:  56%|███████████████▊            | 563/1000.0 [00:33<00:19, 22.79it/s]0.7079646 0.651 0.313\n",
      "Training:  62%|█████████████████▏          | 616/1000.0 [00:37<00:16, 23.22it/s]0.7079646 0.652 0.315\n",
      "Training:  67%|██████████████████▋         | 668/1000.0 [00:40<00:14, 22.49it/s]0.7079646 0.653 0.317\n",
      "Training:  72%|████████████████████▏       | 720/1000.0 [00:43<00:12, 22.51it/s]0.7079646 0.654 0.319\n",
      "Training:  77%|█████████████████████▋      | 774/1000.0 [00:47<00:09, 22.95it/s]0.7079646 0.654 0.319\n",
      "Training:  82%|███████████████████████     | 825/1000.0 [00:50<00:07, 22.49it/s]0.7079646 0.6535 0.318\n",
      "Training:  88%|████████████████████████▌   | 879/1000.0 [00:53<00:05, 23.56it/s]0.7079646 0.654 0.319\n",
      "Training:  95%|██████████████████████████▌ | 947/1000.0 [00:56<00:01, 30.09it/s]0.7079646 0.654 0.319\n",
      "Training: 100%|███████████████████████████▉| 999/1000.0 [01:00<00:00, 16.55it/s]\n",
      "INFO:root:Merging best checkpoints into single model...\n",
      "\n",
      "\n",
      "INFO:root:\n",
      "################\n",
      "Final Model Accuracy: 0.637499988079071\n",
      "Final Model Recall: 0.2809999883174896\n",
      "Final Model False Positives per Hour: 0.44247788190841675\n",
      "################\n",
      "\n",
      "DEBUG:tensorflow:Falling back to TensorFlow client; we recommended you install the Cloud TPU client directly with pip install cloud-tpu-client.\n",
      "DEBUG:h5py._conv:Creating converter from 7 to 5\n",
      "DEBUG:h5py._conv:Creating converter from 5 to 7\n",
      "DEBUG:h5py._conv:Creating converter from 7 to 5\n",
      "DEBUG:h5py._conv:Creating converter from 5 to 7\n",
      "2023-09-04 11:11:12.446476: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-09-04 11:11:12.446795: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2023-09-04 11:11:12.446847: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2023-09-04 11:11:12.446979: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "WARNING:absl:Function `__call__` contains input name(s) onnx_tf__tf_Flatten_0_45bfc89f with unsupported characters which will be renamed to onnx_tf__tf_flatten_0_45bfc89f in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-09-04 11:11:13.859582: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "WARNING:absl:Found untraced functions such as gen_tensor_dict while saving (showing 1 of 1). These functions will not be directly callable after loading.\n",
      "INFO:tensorflow:Assets written to: /tmp/tmpqvff79vt/tf_model/assets\n",
      "2023-09-04 11:11:14.012590: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:357] Ignored output_format.\n",
      "2023-09-04 11:11:14.012606: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:360] Ignored drop_control_dependency.\n",
      "2023-09-04 11:11:14.013126: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: /tmp/tmpqvff79vt/tf_model\n",
      "2023-09-04 11:11:14.013536: I tensorflow/cc/saved_model/reader.cc:78] Reading meta graph with tags { serve }\n",
      "2023-09-04 11:11:14.013546: I tensorflow/cc/saved_model/reader.cc:119] Reading SavedModel debug info (if present) from: /tmp/tmpqvff79vt/tf_model\n",
      "2023-09-04 11:11:14.014464: I tensorflow/cc/saved_model/loader.cc:228] Restoring SavedModel bundle.\n",
      "2023-09-04 11:11:14.025569: I tensorflow/cc/saved_model/loader.cc:212] Running initialization op on SavedModel bundle at path: /tmp/tmpqvff79vt/tf_model\n",
      "2023-09-04 11:11:14.030851: I tensorflow/cc/saved_model/loader.cc:301] SavedModel load for tags { serve }; Status: success: OK. Took 17727 microseconds.\n",
      "2023-09-04 11:11:14.036528: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-09-04 11:11:14.046951: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:1963] Estimated count of arithmetic ops: 0.101 M  ops, equivalently 0.050 M  MACs\n",
      "\n",
      "Estimated count of arithmetic ops: 0.101 M  ops, equivalently 0.050 M  MACs\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Step 3: Train model\n",
    "\n",
    "!{sys.executable} openwakeword/openwakeword/train.py --training_config my_model.yaml --train_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ba9c43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openwakeword_dev",
   "language": "python",
   "name": "openwakeword_dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
